# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vvtOAabLNSXejZH0fPdvHmIuqOqYmgCU
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
!pip3 install spacy
!pip3 install -U scikit-learn
!pip3 install seaborn

import numpy as np
import pandas as pd

train = pd.read_json('./cyberTrolls.json', lines = True)

def clean_text(text):
    text = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", text)
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "cannot ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r",", " ", text)
    text = re.sub(r"\.", " ", text)
    text = re.sub(r"!", " ! ", text)
    text = re.sub(r"\/", " ", text)
    text = re.sub(r"\^", " ^ ", text)
    text = re.sub(r"\+", " + ", text)
    text = re.sub(r"\-", " - ", text)
    text = re.sub(r"\=", " = ", text)
    text = re.sub(r"'", " ", text)
    text = re.sub(r":", " : ", text)
    text = re.sub(r"(\d+)(k)", r"\g<1>000", text)
    text = re.sub(r" e g ", " eg ", text)
    text = re.sub(r" b g ", " bg ", text)
    text = re.sub(r" u s ", " american ", text)
    text = re.sub(r" 9 11 ", "911", text)
    text = re.sub(r"e - mail", "email", text)
    text = re.sub(r"j k", "jk", text)
    text = re.sub(r"\s{2,}", " ", text)
    return text

len(train)

train1.head()

"""First of all, after looking at all of extras, its all NaN, does not add any value and therefor will be removed"""

len(train) - train.extras.isnull().count()

train.drop(['extras'], axis=1, inplace=True)

train.head()

train.annotation.head()

def right(value, count):
    # To get right part of string, use negative first index in slice.
    return value[-count:]

# Test the method.
source = "soft orange cat"
print(right(source, 3))
print(right(source, 1))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# strs = []
# for i in range(len(train.annotation)):
#     strs.append(str(train.annotation[i]))

strs[:5]

strs[0][-4]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# results = []
# for i in range(len(strs)):
#     results.append(strs[i][-4])

percent_of_0 = count/len(results)
percent_of_1 = 1 - percent_of_0
percent_of_0  * 100

percent_of_1

len(results)

len(train.annotation)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# results_nums = []
# for i in range(len(results)):
#     results_nums.append(int(results[i]))

type(results_nums[0])

train.annotation = results_nums

1000000/20000

# a milion annotations would take half a second, fast enough
8 * 50 / 1000

def get_label_percent(data):
    count = 0
    for i in data.annotation:
        if i == 0:
            count += 1
#     return count
    percent_of_o = len(data)/count
    percent_of_1 = 1 - percent_of_0
    print('percent of Non Offensive: {}'.format(percent_of_0 * 100))
    print('percent of Offensive: {}'.format(percent_of_1 * 100))

get_label_percent(train)

"""### Text classification"""

from fastai import * 
from fastai.text import * 
from fastai.core import *

# batch size
bs = 256

train.head()

"""This is the first part of creatin a language model in fastai, getting the data."""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Get the data from a pandas dataframe, in the train df, specified path and the content is in column 0
# data_lm = (TextList.from_df(train, path='.', cols=0)            
#            # split randomlly 20% to validation
#             .split_by_rand_pct(0.2)
#            # language model labeling
#             .label_for_lm()         
#            # load a data bunch each time, not everything together as some data will be too big 
#             .databunch(bs=bs))

"""Train a model with the data we just gathered, LSTM model and a dropout of 0.3"""

# Commented out IPython magic to ensure Python compatibility.
# %time learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)

"""Training the model with one cycle, lr of 1e-2, momentum that runs between 0.8 and 0.7. We get a ~0.18 accuracy which is reasonable considering the amount of training and little data. This is a language model, its trained to predict the next word. We will use it now (and later differently) just as a model that understands a bit English"""

# Commented out IPython magic to ensure Python compatibility.
# %time learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))

data_lm.save('data_lm.pkl')

data_lm = load_data('.', 'data_lm.pkl', bs=bs)

"""This is how the model sees the text. With some special tokens in some places that helps the model understand the text better"""

data_lm.show_batch()

"""A classifier, that has the same vocab as the language model"""

learn.save_encoder('fine_tuned_enc')

data_clas = (TextList.from_df(train, path='.', cols=0, vocab=data_lm.vocab)
            .split_by_rand_pct(0.2)
            .label_from_df(cols=1)
            .databunch(bs=bs))

data_clas.save('data_clas.pkl')

data_clas = load_data('.', 'data_clas.pkl', bs=bs)

data_clas.show_batch()

learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)
learn.load_encoder('fine_tuned_enc')

# Commented out IPython magic to ensure Python compatibility.
# %time learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))

learn.predict('I love you')

learn.predict('I hate you')

learn.predict('you are such an ugly, hated person')

hateSpeech = ['I hate you', 'you are terrible', 'you suck', 'go away', 'I hope you die']

niceComemnts = ['I love you', 'you are amazing', 'You are wonderful', 'I hope you have a great life', 'I wish you the best']

def calculate_offensive_score(data):
    score = []
    avg = []
    for i in range(len(data)):
        score.append(learn.predict(data[i])[2])
    for i in range(len(score)):
        avg.append(score[i][1].item())
    return statistics.mean(avg)

"""As we can see, even with very little data, the model can recognize good to bad and gives the hateSpeech an higher offensive score"""

calculate_offensive_score(hateSpeech)

calculate_offensive_score(niceComemnts)

"""## Classical NLP

The data could be analzyed all together, but I think it will be more insigtful to seperate the offensive and non offensive data and generate the same analysis on both of groups. This approach will let us eaiser understand the differences in nature of offensive/non offensive comments
"""

train.head()

neg_data = train[train.annotation == 1]

pos_data = train[train.annotation == 0]

# As we've seen before, data split is not 100% balanced
len(neg_data), len(pos_data)

neg_data.head()

pos_data.head()

from sklearn.feature_extraction.text import CountVectorizer
import spacy
spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS

"""Sometimes I prefer to make the entire class run with one/two commands (as I will do in the wordcloud's case), here I will seperate the different operations mostly to demonstrate the alternative"""

class TopWords():
    def __init__(self, corpus, words, num_of_words, clean_words=[]):
        self.corpus = corpus
        self.words = words
        self.clean_words = clean_words,
        self.num_of_words = num_of_words
        
    def get_top_n_words(self, corpus, n):
        vec = CountVectorizer().fit(self.corpus)
        bag_of_words = vec.transform(self.corpus)
        sum_words = bag_of_words.sum(axis=0) 
        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
        self.words = words_freq[:self.words]
    
    def wrds(self):
        clean_words = []
        for i in self.words:
            if i[0] not in spacy_stopwords:
                clean_words.append(i)
        self.clean_words = clean_words[:self.num_of_words]
        return self.clean_words
    
    def plot(self):
        x,y = [], []
        for i in self.clean_words:
            x.append(i[0])
            y.append(i[1])
        plt.bar(x, y)

pos1 = TopWords(pos_data.content, 200, 5)

pos1.get_top_n_words(pos1.corpus, pos1.words)

pos1.wrds()

neg1 = TopWords(neg_data.content, 200, 5)

neg1.get_top_n_words(neg1.corpus, neg1.words)

neg1.wrds()

"""Perhaps surprising, the two most common words are hate and damn. In the non offensive twits, hate even appears more. Assuming the data was labeled correctly, this means a word like hate does not neccessarily mean the tweet is offensive. Digging deeper however, does show significant differences in the tweets, where like and lol appears in one group, and fuck, and ass appear in the second group for same occuracy rank. This very shallow example shows that if we would be searching for offensive/non offensive tweets, filtering by hate or damn would not do the trick, where as like/lol vs fuck/ass would. It also sheds some light on the way people write, the fact that I wrote hate, damn and sucks in one sentece, does not mean this is offensive. This might be counterintuitive."""

# This example is suitable for 5 words, if you chose more, adjust figsize/subplots/fontsize
plt.figure(figsize=(30, 5))

plt.subplot(132)
plt.title('Offensive', fontsize=20)
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)
plt.xlabel('most common words', fontsize=20, color='green')
plt.ylabel('count of occurrences', fontsize=20, color='green')
neg1.plot()

plt.subplot(131)
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)
plt.title('Non Offensive', fontsize=20)
plt.xlabel('most common words', fontsize=20, color='green')
plt.ylabel('count of occurrences', fontsize=20, color='green')
pos1.plot()

from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import CountVectorizer
stop_words = text.ENGLISH_STOP_WORDS

!pip3 install wordcloud

from wordcloud import STOPWORDS
stopwords = set(STOPWORDS)

class GenerateWordCloud:
    def __init__(self, corpus):
        self.corpus = corpus
        
    def __call__(self):
        self.processed_data = ''.join(self.corpus)
        self.wc = WordCloud(stopwords=stopwords, width=500, height=200).generate( self.processed_data)
        plt.imshow(self.wc)
        plt.show()

pos_wc = GenerateWordCloud(pos_data.content)

pos_wc()

neg_wc()

from collections import Counter

class UniqueWords():
    
    def __init__(self, corpus):
        self.corpus = corpus
        
    def __call__(self):
        unique_words = set(self.corpus.split())
        percent_unique_words = len(unique_words)/len(self.corpus)
        return percent_unique_words

percent_pos_words = UniqueWords(pos_text)
pp = percent_pos_words()
pp

percent_neg_words = UniqueWords(neg_text)
pn = percent_neg_words()
pn

"""#### Non offensive comments use 1.8 times more unqiue words."""

pp/pn

"""### Polarity/Subjectivity"""

!pip3 install TextBlob

"""Some confirmation/different view on the labels. I will show the polarity of the text, when from -1 (very negative) to 1 (very positive). We should expect a higher number when the test is more positive and vice versa"""

pol = lambda x: TextBlob(x).sentiment.polarity
sub = lambda x: TextBlob(x).sentiment.subjectivity

pos_pol = pos_data.content.apply(pol)
neg_pol = neg_data.content.apply(pol)

"""No surprises here"""

pos_pol.mean(), neg_pol.mean()

pos_sub = pos_data.content.apply(sub)
neg_sub = neg_data.content.apply(sub)

"""As to subjectivity, we see no big difference. While 1 being highly subjecive and 0 being much more objective, seems like offensive tweets are more opinionated, but still, with a rather log gap"""

pos_sub.mean(), neg_sub.mean()

"""### Topic Modeling"""

!pip3 install gensim

from gensim import matutils, models
import scipy.sparse

cv = CountVectorizer(stop_words = stop_words)
data_cv = cv.fit_transform(train.content)
data_stop = pd.DataFrame(data_cv.toarray(), columns = cv.get_feature_names())
data_stop.index = train.index

subjects_train = data_stop.transpose()

sparse_counts = scipy.sparse.csr_matrix(subjects)
corpus = matutils.Sparse2Corpus(sparse_counts)

id2word = dict((v, k) for k, v in cv.vocabulary_.items())

# Commented out IPython magic to ensure Python compatibility.
# %time lda = models.LdaModel(corpus = corpus, id2word = id2word, num_topics = 2, passes = 10)
lda.print_topics()

# Commented out IPython magic to ensure Python compatibility.
# %time lda = models.LdaModel(corpus = corpus, id2word = id2word, num_topics = 10, passes = 100)
lda.print_topics()

"""### Word2Vector"""

tokens = []
def gen_tokens(data):
    lines = []
    for line in data:
        lines.append(line)
    for line in lines:
        tokens.append(gensim.utils.simple_preprocess(line))

gen_tokens(test.content)

del tokens

# comment this two out if you want to the gensim logs
import logging, sys
logging.disable(sys.maxsize)

import gzip
import gensim 
# import logging
# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

model = gensim.models.Word2Vec(tokens, size=150, window=10, min_count=2, workers=10)
model.train(tokens,total_examples=len(t),epochs=10)

modelTest = gensim.models.Word2Vec (t, size=300, window=2, min_count=20, workers=10)

import seaborn as sns
sns.set_style("darkgrid")
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

def plot_w2v(model, word, list_names):
    """ Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,
    its list of most similar words, and a list of words.
    """
    arrays = np.empty((0, 300), dtype='f')
    word_labels = [word]
    color_list  = ['red']

    # adds the vector of the query word
    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)
    
    # gets list of most similar words
    close_words = model.wv.most_similar([word])

    
    # adds the vector for each of the closest words to the array
    for wrd_score in close_words:
        wrd_vector = model.wv.__getitem__([wrd_score[0]])
        word_labels.append(wrd_score[0])
        color_list.append('blue')
        arrays = np.append(arrays, wrd_vector, axis=0)
        
    #     adds the vector for each of the words from list_names to the array
    for wrd in list_names:
        wrd_vector = model.wv.__getitem__([wrd])
        word_labels.append(wrd)
        color_list.append('green')
        arrays = np.append(arrays, wrd_vector, axis=0)

        
    # Reduces the dimensionality from 300 to 50 dimensions with PCA
    reduc = PCA(n_components=10).fit_transform(arrays)
    
    # Finds t-SNE coordinates for 2 dimensions
    np.set_printoptions(suppress=True)
    
    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)
    
    # Sets everything up to plot
    df = pd.DataFrame({'x': [x for x in Y[:, 0]],
                       'y': [y for y in Y[:, 1]],
                       'words': word_labels,
                       'color': color_list})
    
    fig, _ = plt.subplots()
    fig.set_size_inches(9, 9)
    
    # Basic plot
    p1 = sns.regplot(data=df,
                     x="x",
                     y="y",
                     fit_reg=False,
                     marker="o",
                     scatter_kws={'s': 40,
                                  'facecolors': df['color']
                                 }
                    )
    
    # Adds annotations one by one with a loop
    for line in range(0, df.shape[0]):
         p1.text(df["x"][line],
                 df['y'][line],
                 '  ' + df["words"][line].title(),
                 horizontalalignment='left',
                 verticalalignment='bottom', size='medium',
                 color=df['color'][line],
                 weight='normal'
                ).set_size(15)

    
    plt.xlim(Y[:, 0].min()-10, Y[:, 0].max()+10)
    plt.ylim(Y[:, 1].min()-10, Y[:, 1].max()+10)
            
    plt.title(('W2v visualization for the word: {}'.format(word)), fontsize=15)

# get some words that are uncorrelated with the chosen word
def get_words(word):
    return[i[0] for i in modelTest.wv.most_similar(negative=[word])]

"""The blue words are the words the model thinks are alike to the chosen word. The green words are words uncorrelated with the chosen words. If the model makes sense, the blue words will be somehow related while the green words will not."""

plot_w2v(modelTest, 'lol', get_words('lol'))

word_model = gensim.models.Word2Vec(sentences, size=100, min_count=1, 

                                    window=5, iter=100)

pretrained_weights = word_model.wv.syn0

len(pretrained_weights)

"""# LSTM"""

import numpy as np
import torch
from torch import nn
import torch.nn.functional as F

def clean_text(text):
    text = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", text)
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "cannot ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r",", " ", text)
    text = re.sub(r"\.", " ", text)
    text = re.sub(r"!", " ! ", text)
    text = re.sub(r"\/", " ", text)
    text = re.sub(r"\^", " ^ ", text)
    text = re.sub(r"\+", " + ", text)
    text = re.sub(r"\-", " - ", text)
    text = re.sub(r"\=", " = ", text)
    text = re.sub(r"'", " ", text)
    text = re.sub(r":", " : ", text)
    text = re.sub(r"(\d+)(k)", r"\g<1>000", text)
    text = re.sub(r" e g ", " eg ", text)
    text = re.sub(r" b g ", " bg ", text)
    text = re.sub(r" u s ", " american ", text)
    text = re.sub(r" 9 11 ", "911", text)
    text = re.sub(r"e - mail", "email", text)
    text = re.sub(r"j k", "jk", text)
    text = re.sub(r"\s{2,}", " ", text)
    return text

b = test.content[:20000]

type(b)

text = ' '.join(b)

# x = []
# for word in b:
#     x.append(word)

# text = ''.join(test['content'])

type(text)

# text = ' '.join(x)

text = clean_text(text)

type(text)

len(text)

with open('./cyberTrolls.json', 'r') as f:
    text = f.read()

encodedVecs = np.array([modelTest[ch] for ch in text])

chars = tuple(set(text))

int2char = dict(enumerate(chars))

char2int = {ch: ii for ii, ch in int2char.items()}

encoded = np.array([char2int[ch] for ch in text])

len(chars)

def one_hot_encode(arr, n_labels):
    
    # Initialize the the encoded array
    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)
    
    # Fill the appropriate elements with ones
    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.
    
    # Finally reshape it to get back to the original array
    one_hot = one_hot.reshape((*arr.shape, n_labels))
    
    return one_hot

def get_batches(arr, n_seqs, n_steps):
    '''Create a generator that returns batches of size
       n_seqs x n_steps from arr.
       
       Arguments
       ---------
       arr: Array you want to make batches from
       n_seqs: Batch size, the number of sequences per batch
       n_steps: Number of sequence steps per batch
    '''
    
    batch_size = n_seqs * n_steps
    n_batches = len(arr)//batch_size
    
    # Keep only enough characters to make full batches
    arr = arr[:n_batches * batch_size]
    
    # Reshape into n_seqs rows
    arr = arr.reshape((n_seqs, -1))
    
    for n in range(0, arr.shape[1], n_steps):
        
        # The features
        x = arr[:, n:n+n_steps]
        
        # The targets, shifted by one
        y = np.zeros_like(x)
        
        try:
            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]
        except IndexError:
            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]
        yield x, y

batches = get_batches(encoded, 10, 50)
x, y = next(batches)

print('x\n', x[:10, :10])
print('\ny\n', y[:10, :10])

class CharRNN(nn.Module):
    
    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,
                               drop_prob=0.5, lr=0.001):
        super().__init__()
        self.drop_prob = drop_prob
        self.n_layers = n_layers
        self.n_hidden = n_hidden
        self.lr = lr
        
        # Creating character dictionaries
        self.chars = tokens
        self.int2char = dict(enumerate(self.chars))
        self.char2int = {ch: ii for ii, ch in self.int2char.items()}
        
        ## Define the LSTM
        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, 
                            dropout=drop_prob, batch_first=True)
        
        ## Define a dropout layer
        self.dropout = nn.Dropout(drop_prob)
        
        ## Define the final, fully-connected output layer
        self.fc = nn.Linear(n_hidden, len(self.chars))
        
        # Initialize the weights
        self.init_weights()
      
    
    def forward(self, x, hc):
        ''' Forward pass through the network. 
            These inputs are x, and the hidden/cell state `hc`. '''
        
        ## Get x, and the new hidden state (h, c) from the lstm
        x, (h, c) = self.lstm(x, hc)
        
        ## Ppass x through the dropout layer
        x = self.dropout(x)
        
        # Stack up LSTM outputs using view
        x = x.view(x.size()[0]*x.size()[1], self.n_hidden)
        
        ## Put x through the fully-connected layer
        x = self.fc(x)
        
        # Return x and the hidden state (h, c)
        return x, (h, c)
    
    
    def predict(self, char, h=None, top_k=None):
        ''' Given a character, predict the next character.
        
            Returns the predicted character and the hidden state.
        '''
#         if cuda:
#             self.cuda()
#         else:
#             self.cpu()
        
        if h is None:
            h = self.init_hidden(1)
        
        x = np.array([[self.char2int[char]]])
        x = one_hot_encode(x, len(self.chars))
        
        inputs = torch.from_numpy(x)
        
#         if cuda:
#             inputs = inputs.cuda()
        
        h = tuple([each.data for each in h])
        out, h = self.forward(inputs, h)

        p = F.softmax(out, dim=1).data
        
#         if cuda:
#             p = p.cpu()
        
        if top_k is None:
            top_ch = np.arange(len(self.chars))
        else:
            p, top_ch = p.topk(top_k)
            top_ch = top_ch.numpy().squeeze()
        
        p = p.numpy().squeeze()
        
        char = np.random.choice(top_ch, p=p/p.sum())
            
        return self.int2char[char], h
    
    def init_weights(self):
        ''' Initialize weights for fully connected layer '''
        initrange = 0.1
        
        # Set bias tensor to all zeros
        self.fc.bias.data.fill_(0)
        # FC weights as random uniform
        self.fc.weight.data.uniform_(-1, 1)
        
    def init_hidden(self, n_seqs):
        ''' Initializes hidden state '''
        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,
        # initialized to zero, for hidden state and cell state of LSTM
        weight = next(self.parameters()).data
        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),
                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())

def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):
    ''' Training a network 
    
        Arguments
        ---------
        
        net: CharRNN network
        data: text data to train the network
        epochs: Number of epochs to train
        n_seqs: Number of mini-sequences per mini-batch, aka batch size
        n_steps: Number of character steps per mini-batch
        lr: learning rate
        clip: gradient clipping
        val_frac: Fraction of data to hold out for validation
        cuda: Train with CUDA on a GPU
        print_every: Number of steps for printing training and validation loss
    
    '''
    
    net.train()
    
    opt = torch.optim.Adam(net.parameters(), lr=lr)
    
    criterion = nn.CrossEntropyLoss()
    
    # create training and validation data
    val_idx = int(len(data)*(1-val_frac))
    data, val_data = data[:val_idx], data[val_idx:]
    
#     if cuda:
#         net.cuda()
    
    counter = 0
    n_chars = len(net.chars)
    
    for e in range(epochs):
        
        h = net.init_hidden(n_seqs)
        
        for x, y in get_batches(data, n_seqs, n_steps):
            
            counter += 1
            
            # One-hot encode our data and make them Torch tensors
            x = one_hot_encode(x, n_chars)
            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)
            
#             if cuda:
#                 inputs, targets = inputs.cuda(), targets.cuda()

            # Creating new variables for the hidden state, otherwise
            # we'd backprop through the entire training history
            h = tuple([each.data for each in h])

            net.zero_grad()
            
            output, h = net.forward(inputs, h)
            
            loss = criterion(output, targets.view(n_seqs*n_steps))

            loss.backward()
            
            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
            nn.utils.clip_grad_norm_(net.parameters(), clip)

            opt.step()
            
            if counter % print_every == 0:
                
                # Get validation loss
                val_h = net.init_hidden(n_seqs)
                val_losses = []
                
                for x, y in get_batches(val_data, n_seqs, n_steps):
                    
                    # One-hot encode our data and make them Torch tensors
                    x = one_hot_encode(x, n_chars)
                    x, y = torch.from_numpy(x), torch.from_numpy(y)
                    
                    # Creating new variables for the hidden state, otherwise
                    # we'd backprop through the entire training history
                    val_h = tuple([each.data for each in val_h])
                    
                    inputs, targets = x, y
#                     if cuda:
#                         inputs, targets = inputs.cuda(), targets.cuda()

                    output, val_h = net.forward(inputs, val_h)
                    val_loss = criterion(output, targets.view(n_seqs*n_steps))
                
                    val_losses.append(val_loss.item())
                
                print("Epoch: {}/{}...".format(e+1, epochs),
                      "Step: {}...".format(counter),
                      "Loss: {:.4f}...".format(loss.item()),
                      "Val Loss: {:.4f}".format(np.mean(val_losses)))

net = CharRNN(chars, n_hidden=512, n_layers=2)

print(net)

# Commented out IPython magic to ensure Python compatibility.
n_seqs, n_steps = 128, 100

# %time train(net, encoded, epochs=1, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, print_every=10)

def sample(net, size, prime='The', top_k=None):
        
#     if cuda:
#         net.cuda()
#     else:
#         net.cpu()

    net.eval()
    
    # First off, run through the prime characters
    chars = [ch for ch in prime]
    
    h = net.init_hidden(1)
    
    for ch in prime:
        char, h = net.predict(ch, h, top_k=top_k)

    chars.append(char)
    
    # Now pass in the previous character and get a new one
    for ii in range(size):
        
        char, h = net.predict(chars[-1], h, top_k=top_k)
        chars.append(char)

    return ''.join(chars)

print(sample(net, 2000, prime='Anna', top_k=5))

"""# W2V Fastai"""

train.head()

data = TextClasDataBunch.from_df('.', train, train)
data.show_batch()

data.vocab.itos[:10]

data.train_ds[0][0]

data.train_ds[0][0].data[:10]

data.train_ds

train.columns

data_lm = (TextList.from_df(train, path='.', cols=0)            
            .split_by_rand_pct(0.2)
            .label_for_lm()           
            .databunch(bs=bs))

bs = 48

data_lm.show_batch()

learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)

# %time learn.lr_find()

# learn.recorder.plot()

learn.fit_one_cycle(1, 1e-15, moms=(0.8,0.7))

learn.unfreeze()

learn.fit_one_cycle(1, 1e-3, moms=(0.8,0.7))

TEXT = "This is really great"
N_WORDS = 40
N_SENTENCES = 2

print("\n".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))

